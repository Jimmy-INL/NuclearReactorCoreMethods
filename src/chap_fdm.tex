%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Finite Difference Methods}
\label{chap:fdm} % Always give a unique label
% use \chaptermark{}
% to alter or adjust the chapter heading in the running head

\abstract*{Each chapter should be preceded by an abstract (10--15 lines long) that summarizes the content. The abstract will appear \textit{online} at \url{www.SpringerLink.com} and be available with unrestricted access. This allows unregistered users to read the abstract as a teaser for the complete chapter. As a general rule the abstracts will not appear in the printed version of your book unless it is the style of your particular book or that of the series to which your book belongs.
Please use the 'starred' version of the new Springer \texttt{abstract} command for typesetting the text of the online abstracts (cf. source file of this chapter template \texttt{abstract}) and include them with the source files of your manuscript. Use the plain \texttt{abstract} command if the abstract is also to appear in the printed version of the book.}


\section{Taylor Series}
\label{sec:fdm_taylor}

% Reference Chapre and Canale
% Simple intro to Taylor Series
% Talk about truncation error

The finite difference method relies heavily on the mathematical concept of 
Taylor Series.\index{Taylor Series}  If we take a function, $f(x)$, the 
independent variable $x$ can be discretized into many points as shown in Figure \_.
If the value of the function is known at $x_{i}$, the value at $x_{i+1}$ can be
determined by a Taylor series expansion at $x_{i}$,
\begin{equation}
     f\left(x_{i+1}\right) = f\left(x_{i}\right) + f^{\prime}\left(x_{i}\right)h + 
     \frac{f^{\prime\prime}\left(x_{i}\right)}{2!}h^{2} + 
     \frac{f^{\left(3\right)}\left(x_{i}\right)}{3!}h^{3}+\cdot\cdot\cdot + 
     \frac{f^{\left(n\right)}\left(x_{i}\right)}{2!}h^{n} + \cdot\cdot\cdot
  \label{eq:fdm_taylor}
\end{equation}
In Eq. (\ref{eq:fdm_taylor}), $f^{\left(3\right)}$ represents the $n$-th derivative of 
the function and $h$ is the spacing between points, $h = x_{i+1} - x_{i}$.
\par
The expansion shown above is exact if the number of terms in the Taylor series
expansion is taken to infinity. Of course, this is not practical for computational
methods and therefore we truncate the series at a finite number of terms. The error
present caused by the truncation is known as truncation error.\index{truncation error}
Instead of representing the full Taylor expansion of a function, we will truncate
the expression after a few number of terms and repesent the truncation error with
$\mathcal{O}\left(h^{n}\right)$. In this representation of the truncation error,
$n$ represents the order of convergence.\index{order of convergence} Order of
convergence means that as the grid is refined by a factor of two for example, the
truncation error will reduce on the order of $2^{n}$. This does not imply that
one method is better than the order, just merely a concept of convergence rate
due to truncation effects. Linear convergence is when $n=1$, quadratic when $n=2$
and cubic when $n=3$. For example, if we expand a function to second order, we
would rewrite Eq. (\ref{eq:fdm_taylor}) this as
\begin{equation}
     f\left(x_{i+1}\right) = f\left(x_{i}\right) + f^{\prime}\left(x_{i}\right)h + 
     \frac{f^{\prime\prime}\left(x_{i}\right)}{2!}h^{2} + \mathcal{O}\left(h^{3}\right).
\end{equation}
As we approximate differentials, we can keep track of this truncation error to determine
order of convergence of our methods. This is one way to ensure that our discretization
method and implementation of solution algorithms are correct.

\section{Approximation of First Derivatives}
\label{sec:fdm_approx}

% Reference Chapre and Canale
% Forward, backward and central first order and second order differentials
% Simple example of Taylor expansion approximating with these approxs

There are many different approximations of differentials that can be constructed based
on Taylor series.  We will first consider the approximation of first order derivatives.
The first approximation is a \emph{first order forward difference} where we use information 
about a point just to the right, $x_{i+1}$, to infer the derivative at $x_{i}$.  If we 
perform a Taylor expansion about point $x_{i+1}$ to first order we get
\begin{equation}
     f\left(x_{i+1}\right) = f\left(x_{i}\right) + f^{\prime}\left(x_{i}\right)h + \mathcal{O}\left(h^{2}\right).
\end{equation}
This equation can be solved for the derivative of the function at $x_{i}$ 
\begin{equation}
     \boxed{f^{\prime}_{for}\left(x_{i}\right) = \frac{f\left(x_{i+1}\right) - f\left(x_{i}\right)}{h} - \mathcal{O}\left(h\right)},
  \label{eq:first_forward}
\end{equation}
where $f^{\prime}_{for}\left(x_{i}\right)$ represents the first order forward difference approximation to the derivative at
$x_{i}$.
\par
The opposite approximation is to consider a point to the left, $x_{i-1}$, to infer the
derivative at $x_{i}$, \emph{known as the first order backward difference}. Here, we take a Taylor expansion to the left,
\begin{equation}
     f\left(x_{i-1}\right) = f\left(x_{i}\right) - f^{\prime}\left(x_{i}\right)h + \mathcal{O}\left(h^{2}\right).
\end{equation}
Solving for the derivative we can arrive at
\begin{equation}
     \boxed{f^{\prime}_{bac}\left(x_{i}\right) = \frac{f\left(x_{i}\right) - f\left(x_{i-1}\right)}{h} + \mathcal{O}\left(h\right)}.
  \label{eq:first_backward}
\end{equation}
Comparing Eqs. (\ref{eq:first_forward}) and (\ref{eq:first_backward}) we see that the formulation looks the same in that it
is always the right point minus the left point in the numerator of the fraction. The only difference is the sign in the 
truncation error is reversed. Therefore, we can expect that one of these approximations will under-predict the true answer
and the other one will over-predict. Again both of these methods are first order methods.
\par 
The last simple approximation of a first derivative is a \emph{second-order central difference}. In this method we look 
at both left and right points. We can Taylor expand each of these to second order to get
\begin{eqnarray}
    f\left(x_{i+1}\right) = f\left(x_{i}\right) + f^{\prime}\left(x_{i}\right)h + \frac{f^{\prime\prime}\left(x_{i}\right)}{2!}h^{2} + \mathcal{O}\left(h^{3}\right) \\
    f\left(x_{i-1}\right) = f\left(x_{i}\right) - f^{\prime}\left(x_{i}\right)h + \frac{f^{\prime\prime}\left(x_{i}\right)}{2!}h^{2} - \mathcal{O}\left(h^{3}\right).
\end{eqnarray}
Subtracting the $x_{i-1}$ equation from the $x_{i+1}$, we are left with
\begin{equation}
    f\left(x_{i+1}\right) - f\left(x_{i-1}\right) = 2f^{\prime}\left(x_{i}\right)h  + \mathcal{O}\left(h^{3}\right).
\end{equation}
Solving for the derivative at $x_{i}$ we arrive at the second order central difference approximation
\begin{equation}
    \boxed{f^{\prime}_{cen}\left(x_{i}\right) = \frac{f\left(x_{i+1}\right) - f\left(x_{i-1}\right)}{2h} - \mathcal{O}\left(h^{2}\right)}.
\end{equation}
From the resulting expression, this approximation method does not depend on the value of the function at $x_{i}$ and that
the scheme is second order convergent.
\par
\begin{figure}[t]
\sidecaption[t]
\scalebox{0.5}{\input{./figs/chap_fdm/convergence.tikz}}
\caption{Convergence rate of forward, backward and central difference approximations. The slope of the error as a function of mesh spacing
is an estimate of the order of convergence of an approximation scheme.}
\label{fig:fdm_convergence}       % Give a unique label
\end{figure}
\paragraph{Example - Order of Convergence First Derivative}
As a simple example, we can approximate the derivative of the function, $f\left(x\right) = x^{4}$, at $x=100$ with
each of the above approximations. We can choose an array of spacing values between $x_{i}$ and $x_{i+1}$ and $x_{i-1}$
and $x_{i}$. For each spacing value we compute the estimate of the derivative using the three approximations above.
To characterize the error of each we find the absolute difference between the approximation and the true value of the
derivative at $x=100$. To infer the order of convergence, we can graph the errors as a function of spacing on a 
log-log scale. These convergence plots are shown in Fig. \ref{fig:fdm_convergence}.
\par 
There are two distinct convergence trends present in Fig. \ref{fig:fdm_convergence}.  The curve with a slope of 1 on
the log-log scale represents forward and backward finite difference approximations. This shows that these methods
have linear convergence consistent with the truncation error. For the central difference approximation we predicted
that it would have quadratic convergence. We can see from the plot that the magnitude of the slope is 2 on the log-log scale.
MATLAB code to solve generate this plot is included below.
\lstinputlisting{./code/chap_fdm/fdm_approx_1.m}
% Discuss second order derivatives

\section{Approximation of Second Derivatives}

In nuclear reactor physics applications, we also need approximations for second derivatives.  The only difference in 
these approximations is that more points to the left or right of $x_{i}$ need to be included. For the \emph{first order
forward difference} approximation, we write two equations to second order. One equation representating a Taylor expansion
to $x_{i+1}$ and the other to $x_{i+2}$,
\begin{eqnarray}
    f\left(x_{i+1}\right) = f\left(x_{i}\right) + f^{\prime}\left(x_{i}\right)h + \frac{f^{\prime\prime}\left(x_{i}\right)}{2!}h^{2} + \mathcal{O}\left(h^{3}\right) 
  \label{eq:second_forward_1}
\\
    f\left(x_{i+2}\right) = f\left(x_{i}\right) + f^{\prime}\left(x_{i}\right)\left(2h\right) + \frac{f^{\prime\prime}\left(x_{i}\right)}{2!}\left(2h\right)^{2} + \mathcal{O}\left(h^{3}\right).
  \label{eq:second_forward_2}
\end{eqnarray}
Since we are approximating the second derivative, the first derivative needs to be canceled out. To cancel this term out, 
we multiply Eq. (\ref{eq:second_forward_1}) by a 2 and subtract it from Eq. (\ref{eq:second_forward_2}). The resulting 
expression is
\begin{equation}
    f\left(x_{i+2}\right) - 2f\left(x_{i+1}\right) = -f\left(x_{i}\right) + f^{\prime\prime}\left(x_{i}\right)h^{2} + \mathcal{O}\left(h^{3}\right).
\end{equation}
The approximation of the second derivative for a first-order forward difference is therefore
\begin{equation}
    \boxed{f^{\prime\prime}_{for}\left(x_{i}\right) = \frac{f\left(x_{i+2}\right) - 2f\left(x_{i+1}\right) + f\left(x_{i}\right)}{h^{2}} - \mathcal{O}\left(h\right)}.
\end{equation}
\par 
For the \emph{first-order backward finite difference} approximation of the second derivative we Taylor expand the function at
$x_{i-1}$ and $x_{i-2}$ 
\begin{eqnarray}
    f\left(x_{i-1}\right) = f\left(x_{i}\right) - f^{\prime}\left(x_{i}\right)h + \frac{f^{\prime\prime}\left(x_{i}\right)}{2!}h^{2} - \mathcal{O}\left(h^{3}\right) 
  \label{eq:second_backward_1}
\\
    f\left(x_{i-2}\right) = f\left(x_{i}\right) - f^{\prime}\left(x_{i}\right)\left(2h\right) + \frac{f^{\prime\prime}\left(x_{i}\right)}{2!}\left(2h\right)^{2} - \mathcal{O}\left(h^{3}\right).
  \label{eq:second_backward_2}
\end{eqnarray}
Similar to the forward finite difference case, we must eliminate the first derivative term by multiplying 
Eq. (\ref{eq:second_backward_1}) by 2 and subtract from Eq. (\ref{eq:second_backward_2}). This results in the following expression:
\begin{equation}
    f\left(x_{i-2}\right) - 2f\left(x_{i-1}\right) = -f\left(x_{i}\right) + f^{\prime\prime}\left(x_{i}\right)h^{2} - \mathcal{O}\left(h^{3}\right).
\end{equation}
The approximation of the second derivative for a first-order backward difference is therefore
\begin{equation}
    \boxed{f^{\prime\prime}_{bac}\left(x_{i}\right) = \frac{f\left(x_{i}\right) - 2f\left(x_{i-1}\right) + f\left(x_{i-2}\right)}{h^{2}} + \mathcal{O}\left(h\right)}.
\end{equation}
\par 
Lastly, the \emph{second-order central difference} approximation to the second derivative can be derived by performing 
a Taylor expansion at $x_{i-1}$ and $x_{i+1}$ to fourth-order,
\begin{eqnarray}
    f\left(x_{i+1}\right) = f\left(x_{i}\right) + f^{\prime}\left(x_{i}\right)h + \frac{f^{\prime\prime}\left(x_{i}\right)}{2!}h^{2} + \frac{f^{\prime\prime\prime}\left(x_{i}\right)}{3!}h^{3} + \mathcal{O}\left(h^{4}\right) \\
    f\left(x_{i-1}\right) = f\left(x_{i}\right) - f^{\prime}\left(x_{i}\right)h + \frac{f^{\prime\prime}\left(x_{i}\right)}{2!}h^{2} - \frac{f^{\prime\prime\prime}\left(x_{i}\right)}{3!}h^{3} + \mathcal{O}\left(h^{4}\right).
\end{eqnarray}
To eliminate the first derivate term, these two equations can be directly added together resulting in
\begin{equation}
    f\left(x_{i+1}\right) + f\left(x_{i-1}\right) = 2f\left(x_{i}\right) + f^{\prime\prime}\left(x_{i}\right)h^{2} + \mathcal{O}\left(h^{4}\right).
\end{equation}
The approximation of the second derivative for a second-order central difference is
\begin{equation}
    \boxed{f^{\prime\prime}_{cen}\left(x_{i}\right) = \frac{f\left(x_{i+1}\right) -2f\left(x_{i}\right) + f\left(x_{i-1}\right)}{h^{2}} - \mathcal{O}\left(h^{2}\right)}.
\end{equation}
The second-order central difference will be the main approximation we use for second order derivatives. This is mainly due to the
fact that has quadratic convergence. The other reason is that for a given computational node in a reactor, we think of leakage
occuring to the left and to the right. This leakage term in represented with mathematically by a second derivative and by using
the central difference approximation, we can couple to both the right and left nodes.

\paragraph{Example - Order of Convergence Second Derivative}

Similar to the previous example, we can approximate the second derivative of the function, $f\left(x\right) = 6x^{6} + 
4x^{3} + 8x + 2$

In this example, we will verify the order of convergence for second derivatives with each of the approximations above.
\section{Higher Order Finite Difference}

\section{Nonuniform Spacing}
\label{subsec:fdm_nonuniform}

% Explanation of why important
% redo some of the above approximations

\section{Finite Difference Multigroup Diffusion Equation}

% show derivation for an arbitrary group
% derive for 2-D mesh with interior, reflective and non-reentrant BC